
import sys
import os


os.environ["CUDA_VISIBLE_DEVICES"] = "7"
sys.path.append("./")
sys.path.append("./utils")
sys.path.append("./train")
import gradio as gr
import spacy
import torch
from torch_geometric.data import Data
from PIL import Image
import json
import os
from utils.utils import tokenizer_graph_token
from ChildText.prompt import LABEL_TEMPLATE_SCORE, LABEL_TEMPLATE_TRAIT, LABEL_TEMPLATE_TRAIT_COMMENT, PROMPT_TEMPLATE,PROMPT_TEMPLATE_TRAIT_COMMENT
from train.train_childtext import  build_heterogeneous_graph_string 
from utils.metric import analyze_composition
import torch
torch.autograd.set_detect_anomaly(True)
import os
from serve.server_utils import COMMENT_GENERATE_PROMPT, CRITERIA, ST_emb_text, cl_model_encode, get_graph_encoder, get_pydot_graph_img, get_text_encoder, gold_image_to_plotly, image_to_plotly, load_model, recognize_audio
import json
from utils.constants import GRAPH_TOKEN_INDEX
from utils.conversation import conv_templates, SeparatorStyle
from utils.utils import tokenizer_graph_token

from serve.explian import explain_visualization


dtype = torch.bfloat16

conv_mode = 'conv_childtext'

model_paths = {
    "vicuna-7b-v1.5-score-nolora": {
        "model_path": '/disk/NarGINA/checkpoints/ChildText/score/llaga-vicuna7b-GRACE_512_STv2-mlpv2-no_edgestr-pre_metric-stage2',    
        #"model_path": "/disk/NarGINA/checkpoints/ChildText/llaga-vicuna7b-GRACE_512_ST-mlpv2-no_edgestr",  # æ›¿æ¢ä¸ºä½ çš„å®é™…æ¨¡å‹è·¯å¾„
        "model_base": "/disk/NarGINA/weights/vicuna-7b-v1.5",  # æ›¿æ¢ä¸ºåŸºåº§æ¨¡å‹è·¯å¾„
        "encoder_weight_path": "/disk/NarGINA/contrastive_learning/weights/ChildText/v2/encoder_model_ST_hidden=512_numlayers=4_headnum=8_loss=3.92_weights.pth",
    },    
    "vicuna-7b-v1.5-multi-trait-nolora": {
        "model_path": '/disk/NarGINA/checkpoints/ChildText/score_trait/llaga_vicuna7b_GRACE_512_STv2_mlpv2_no_edgestr_trait/checkpoint-204',    
        #"model_path": "/disk/NarGINA/checkpoints/ChildText/llaga-vicuna7b-GRACE_512_ST-mlpv2-no_edgestr",  # æ›¿æ¢ä¸ºä½ çš„å®é™…æ¨¡å‹è·¯å¾„
        "model_base": "/disk/NarGINA/weights/vicuna-7b-v1.5",  # æ›¿æ¢ä¸ºåŸºåº§æ¨¡å‹è·¯å¾„
        "encoder_weight_path": '/disk/NarGINA/contrastive_learning/weights/ChildText/v2/encoder_model_ST_hidden=512_numlayers=4_headnum=8_loss=3.92_weights.pth',
    },
    # "vicuna-7b-v1.5-multi-trait-nolora": {
    #     "model_path": '/disk/NarGINA/checkpoints/ChildText/llaga_vicuna7b_GRACE_512_7bv2_mlpv2_no_edgestr_trait/checkpoint-192',    
    #     #"model_path": "/disk/NarGINA/checkpoints/ChildText/llaga-vicuna7b-GRACE_512_ST-mlpv2-no_edgestr",  # æ›¿æ¢ä¸ºä½ çš„å®é™…æ¨¡å‹è·¯å¾„
    #     "model_base": "/disk/NarGINA/weights/vicuna-7b-v1.5",  # æ›¿æ¢ä¸ºåŸºåº§æ¨¡å‹è·¯å¾„
    #     "encoder_weight_path": '/disk/NarGINA/contrastive_learning/weights/ChildText/v2/encoder_model_vicuna-7b-v1.5_hidden=512_numlayers=4_headnum=8_loss=4.32_weights.pth',
    # },#TODOè¿˜æœªå®ç°é¢„å¤„ç†å‡½æ•°ï¼ŒåŠ æ–‡æœ¬å‰ç¼€

    "vicuna-7b-v1.5-score": {
        "model_path": "/disk/NarGINA/checkpoints/ChildText/llaga-vicuna7b-GRACE_512_ST-mlpv2-no_edgestr-lora",  # æ›¿æ¢ä¸ºä½ çš„å®é™…æ¨¡å‹è·¯å¾„
        "model_base": "/disk/NarGINA/weights/vicuna-7b-v1.5",  # æ›¿æ¢ä¸ºåŸºåº§æ¨¡å‹è·¯å¾„
        #"encoder_weight_path":
    },    
    "vicuna-7b-v1.5-multi-trait": {
        "model_path": "/disk/NarGINA/checkpoints/ChildText/llaga_vicuna7b_GRACE_512_ST_mlpv2_no_edgestr_trait_premicro_lora",  # æ›¿æ¢ä¸ºä½ çš„å®é™…æ¨¡å‹è·¯å¾„
        "model_base": "/disk/NarGINA/weights/vicuna-7b-v1.5",  # æ›¿æ¢ä¸ºåŸºåº§æ¨¡å‹è·¯å¾„
        #"encoder_weight_path":
    },
}

def get_all_model(select_model_item):
    tokenizer, model = load_model(select_model_item["model_path"], select_model_item["model_base"])
    graph_encoder_model = get_graph_encoder(select_model_item["encoder_weight_path"],select_model_item["model_path"])
    text_encoder = get_text_encoder(select_model_item["model_path"])
    return tokenizer,model,graph_encoder_model,text_encoder

# åŠ è½½ä¼˜å…ˆæ¨¡å‹æ¨¡å‹
first_model_item = list(model_paths.items())[0]
current_model_name = first_model_item[0]

tokenizer,model,graph_encoder_model,text_encoder = get_all_model(first_model_item[1])


# æ˜¾ç¤ºæ•…äº‹å›¾å›¾ç‰‡
image_folder = "/disk/NarGINA/serve/asset/story"  # æ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
# è·å–å¹¶æ’åºå›¾ç‰‡è·¯å¾„ï¼Œç¡®ä¿é¡ºåºä¸º 0.png, 1.png, 2.png...
image_paths = sorted(
    [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith(".png") and 'graph' not in f],
    key=lambda x: int(os.path.splitext(os.path.basename(x))[0])
)
images = [Image.open(path) for path in image_paths]
total_images = len(images)
current_index = 0# å…¨å±€å˜é‡æ¥è·Ÿè¸ªå½“å‰å›¾ç‰‡ç´¢å¼•


custom_css = """
.icon-btn {
    max-width: 30px;
    border: none;
    padding: 5px; /* å‡å°‘å†…è¾¹è· */
    margin: 2px; /* å‡å°‘å¤–è¾¹è·ï¼Œä½¿æŒ‰é’®æ›´ç´§å‡‘ */
}
.textbox-custom {
    font-weight: bold; /* ä½¿æ–‡æœ¬åŠ ç²— */
    font-size: 50px; /* å¯é€‰ï¼šè®¾ç½®å­—ä½“å¤§å° */
}

"""

def model_inference(input_text, model_name,generate_comment):
    response = chat_with_model(input_text, model_name,generate_comment)
    return response

# å®šä¹‰æ¨¡å‹åˆ‡æ¢å‡½æ•°
def change_model(model_name):
    global tokenizer, model, graph_encoder_model,text_encoder,current_model_name
    if model_name != current_model_name:
        #tokenizer, model = load_model(model_paths[model_name]['model_path'], model_paths[model_name]["model_base"])
        tokenizer,model,graph_encoder_model,text_encoder = get_all_model(model_paths[model_name])
        current_model_name = model_name
    return f"å·²åŠ è½½æ¨¡å‹ï¼š{model_name}"

    
def process(input_text,g,tokenizer):
    edge_code_str = build_heterogeneous_graph_string(g.edge_index,g.edge_type)
    if 'trait' in current_model_name :    # å™äº‹å›¾+æ–‡æœ¬ï¼Œå¤šç»´åº¦è¯„åˆ†åŠ è¯„è®º
        qs = PROMPT_TEMPLATE_TRAIT_COMMENT
    elif 'score' in current_model_name :                          # å™äº‹å›¾+æ–‡æœ¬ï¼Œåªåšè¯„åˆ†       
        qs = PROMPT_TEMPLATE

    qs = qs.replace("<essay_text>",input_text)
    ## è®¡ç®—å¾®è§‚æŒ‡æ ‡
    if 'metric' in model_paths[current_model_name]['model_path']:
        nlp_model = spacy.load("zh_core_web_sm")
        micro_metric = analyze_composition(text=input_text,nlp=nlp_model)
        qs = qs.replace("<micro_metrics_str>",micro_metric)
    else:
        qs = qs.replace("3. å¾®è§‚ç»“æ„ç»´åº¦è¯„åˆ†æ—¶ï¼Œè¯·ä½ ä½¿ç”¨ä»¥ä¸‹é‡åŒ–æ•°æ®ä½œä¸ºå‚è€ƒï¼š<micro_metrics_str>\n","")
    
    if 'trait' in current_model_name and 'comment' in current_model_name:
        label_template = LABEL_TEMPLATE_TRAIT_COMMENT
    elif 'trait' in current_model_name:
        label_template = LABEL_TEMPLATE_TRAIT
    else:
        label_template = LABEL_TEMPLATE_SCORE
    qs = qs.replace("<label_template>",label_template)
    if "no_edgestr" in current_model_name:
        qs = qs.replace("- è¾¹ç»“æ„ï¼š<edge_code_str>\n",'')
    else:
        qs = qs.replace("<edge_code_str>",edge_code_str)

    g.id = id
    g.conversations = [{"from":"human","value":qs},
                    {"from":"gpt","value": None}]

    conv = conv_templates[conv_mode].copy()
    conv.append_message(conv.roles[0], qs)
    conv.append_message(conv.roles[1], None)
    prompt = conv.get_prompt()
    #print(prompt)
    input_ids = tokenizer_graph_token(prompt, tokenizer, GRAPH_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
    g.x = g.x.to(dtype)
    g.edge_attr = g.edge_attr.to(dtype)
    # g_batch = Batch.from_data_list([g])
    graph = torch.LongTensor(range(g.x.size(0))).unsqueeze(0)
    graph_emb = g.x.unsqueeze(0)
    edge_index = g.edge_index
    edge_attr = g.edge_attr.to(dtype)
    edge_type = g.edge_type
    return input_ids,graph_emb,graph,edge_index,edge_attr,edge_type,conv,qs


def graph_json_2_pyg(g_json,encoder):
    # è¾¹ç±»å‹æ–‡æœ¬æè¿°
    relation_descriptions = [
        "ä½¿èƒ½-å› æœ",
        "åŠ¨æœº-å› æœ",
        "ç‰©ç†-å› æœ",
        "å¿ƒç†-å› æœ",
        "å¹¶åˆ—",
    ]
    if 'STv2' in model_paths[current_model_name]['model_path']:
        edge_embeddings = ST_emb_text(relation_descriptions,encoder).cpu()
    else:
        edge_embeddings = encoder.encode(relation_descriptions).cpu()
    # Step 1: è·å–æ‰€æœ‰å”¯ä¸€èŠ‚ç‚¹å¹¶åˆ†é…ç´¢å¼•
    node_dict = {}  # å­˜å‚¨æ¯ä¸ªèŠ‚ç‚¹å’Œå®ƒçš„ç´¢å¼•
    edges = []  # å­˜å‚¨è¾¹
    edge_types = []  # å­˜å‚¨è¾¹çš„ç±»å‹ç´¢å¼•
    edge_attr_list = []  # å­˜å‚¨æ¯ä¸ªè¾¹çš„ç‰¹å¾å‘é‡
    node_texts = []  # å­˜å‚¨æ¯ä¸ªèŠ‚ç‚¹çš„æ–‡æœ¬

    # åˆ†é…ç´¢å¼•å¹¶åˆ›å»ºè¾¹ï¼Œæ”¶é›†èŠ‚ç‚¹æ–‡æœ¬
    for graph in g_json:
        for edge in graph:
            first_event = edge["first_event"]
            second_event = edge["second_event"]
            relation = edge["relation"]

            # å¦‚æœèŠ‚ç‚¹æœªè§è¿‡ï¼Œåˆ†é…ç´¢å¼•å¹¶å­˜å‚¨æ–‡æœ¬
            if first_event not in node_dict:
                node_dict[first_event] = len(node_dict)
                node_texts.append(first_event)  # æ·»åŠ åˆ°æ–‡æœ¬åˆ—è¡¨
            if second_event not in node_dict:
                node_dict[second_event] = len(node_dict)
                node_texts.append(second_event)  # æ·»åŠ åˆ°æ–‡æœ¬åˆ—è¡¨

            # å¤„ç†è¾¹ï¼Œè·³è¿‡"æ— "ç±»å‹çš„è¾¹
            if relation != "æ— ":
                first_idx = node_dict[first_event]
                second_idx = node_dict[second_event]
                edges.append([first_idx, second_idx])

                # å­˜å‚¨è¾¹çš„ç±»å‹ç´¢å¼•
                edge_type = relation_descriptions.index(relation)
                edge_types.append(edge_type)
                
                # æ·»åŠ è¾¹çš„ç‰¹å¾å‘é‡
                edge_attr_list.append(edge_embeddings[edge_type])

    # Step 3: æ„å»ºè¾¹ç´¢å¼•å¼ é‡
    if edges:  # å¦‚æœæœ‰è¾¹
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # è½¬ç½®ä»¥ç¬¦åˆPyGæ ¼å¼
        edge_attr = torch.stack(edge_attr_list)
    else:  # å¦‚æœæ²¡æœ‰è¾¹
        edge_index = torch.empty((2, 0), dtype=torch.long)  # ç©ºè¾¹å¼ é‡
        edge_attr = None

    # Step 4: åˆ›å»ºæ•°æ®å¯¹è±¡
    edge_type_tensor = torch.tensor(edge_types, dtype=torch.long)  # å°†è¾¹ç±»å‹ç´¢å¼•è½¬æ¢ä¸ºå¼ é‡
    if 'STv2' in model_paths[current_model_name]['model_path']:
        node_embeddings = ST_emb_text(node_texts,encoder).cpu()
    else:
        node_embeddings = encoder.encode(node_texts).cpu()
    data = Data(x=node_embeddings, edge_index=edge_index, edge_type=edge_type_tensor, edge_attr=edge_attr)##TODO 7bv2çš„æ•°æ®é›†éœ€è¦å¤šä¸€æ­¥é¢„å¤„ç†ï¼Œç»™æ¯ä¸ªèŠ‚ç‚¹å’Œè¾¹åŠ ä¸Šå‰ç¼€æ–‡æœ¬
    return data


# å®šä¹‰å¯¹è¯å‡½æ•°
def chat_with_model(input_text, model_name,generate_comment):
    
    #tokenizer, model = model_dict.get(model_name)
    #g_json = narrative_graph_api(input_text=input_text)
    with open('/disk/NarGINA/ChildText/gold_data/content.json', 'r', encoding='utf-8') as file:
        g_json = json.load(file)
    with open('/disk/NarGINA/ChildText/gold_data/gold_graph_content.json', 'r', encoding='utf-8') as file:
        gold_g_json = json.load(file)
    g = graph_json_2_pyg(g_json,encoder=text_encoder)
    # data_path = '/disk/NarGINA/dataset/ChildText/embedings/GRACE_7b/pretrained_GAT_hidden=512_test.pkl'
    # with open(data_path, 'rb') as f:
    #     data = pickle.load(f)
    # g = data[0]
    #0:å°ç‹—å°é’è›™å°æœ‹å‹å°±è¦å‡†å¤‡ç¡è§‰äº†.äºæ˜¯å°é’è›™æ²¡ç¡è§‰.ä»–ä»¬ä¸¤ä¸ªç¡è§‰äº†.ç„¶åä»–ä»¬é†’äº†.å’¦?å°é’è›™ä¸è§äº†.ä»–ä»¬åœ¨æ‰¾å°é’è›™.åˆ°å¤„æ‰¾å•Šæ‰¾,æ‰¾å•Šæ‰¾.äºæ˜¯ä»–åœ¨æ‰¾å•Š.ä»–æ‰ä¸‹å»äº†.å°ç‹—æ¥ä½äº†ä»–.ç„¶åä»–ä»¬åœ¨æ‰¾å°é’è›™.åœ°æ´é‡Œæ˜¯ä¸æ˜¯æœ‰å°é’è›™å‘¢?ä¸æ˜¯å°é’è›™.ä»–ä»¬åœ¨æ ‘æ´é‡Œæ‰¾.åˆä¸æ˜¯å°é’è›™.ä»–ä»¬åœ¨å±±ä¸Šæ‰¾.å’Œå°é©¯é¹¿å°±ä¸€èµ·æ‰¾äº†.æ‰ä¸‹å»äº†.ä»–ä»¬åœ¨æ²³é‡Œäº†.ä»–æ‰è¿›æ²³åº•ä¹Ÿè¦å¼€å§‹æ‰¾å°é’è›™.å˜˜.ç„¶åä»–æ‰¾å•Šæ‰¾.å¿½ç„¶æ‰¾åˆ°äº†ä¸¤åªå°é’è›™.è¿˜æœ‰å‡ åªå°é’è›™.ç„¶åä»–æ‰¾åˆ°äº†å°é’è›™.ä»–å°±é«˜å…´çš„ç¬‘äº†.
    # ä½¿ç”¨ to_graph å‡½æ•°ç”Ÿæˆå›¾æ•°æ®
    g = cl_model_encode(g,graph_encoder_model)

    input_ids,graph_emb,graph,edge_index,edge_attr,edge_type,conv,prompt= process(input_text,g,tokenizer)
    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2

    outputs = model_inference_fn(model,tokenizer, input_ids,graph_emb,graph,edge_index,edge_attr,edge_type,stop_str)
    comment_outputs = ''
    if generate_comment:
        conv = conv_templates[conv_mode].copy()
        conv.append_message(conv.roles[0], prompt)
        conv.append_message(conv.roles[1], outputs)
        conv.append_message(conv.roles[0], COMMENT_GENERATE_PROMPT)
        conv.append_message(conv.roles[1], None)
        new_input_text = conv.get_prompt()
        input_ids = tokenizer_graph_token(new_input_text, tokenizer, GRAPH_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
        comment_outputs = model_inference_fn(model,tokenizer, input_ids,graph_emb,graph,edge_index,edge_attr,edge_type,stop_str)
    # iimage generate
    
    # main_filename,isolated_filename = get_pydot_graph_img(g_json,'child')
    gold_graph_filename,gold_isolated_filename = get_pydot_graph_img(gold_g_json,'gold')
    #gold_graph_filename = "/disk/NarGINA/serve/asset/gold.png"
    gold_main_fig = image_to_plotly(gold_graph_filename)


    main_filename = "/disk/NarGINA/serve/asset/explain.png"
    explain_visualization(g_json,gold_g_json,main_filename)
    main_fig = image_to_plotly(main_filename)

    # img = post_graph_image()
    final_output = outputs + '\n' +comment_outputs
    return final_output,main_fig,gold_main_fig

def model_inference_fn(model,tokenizer, input_ids,graph_emb,graph,edge_index,edge_attr,edge_type,stop_str):
    # ç”Ÿæˆè¾“å‡º
    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            graph_emb=graph_emb.to(dtype),
            graph=graph.to('cuda').to(dtype),  # ç®€å•çš„ graph ç¤ºä¾‹
            edge_index=edge_index.to('cuda').to(dtype),
            edge_attr=edge_attr.to('cuda').to(dtype),
            edge_type=edge_type.to('cuda').to(dtype),
            max_new_tokens=1024,
            do_sample=True,
            temperature=1.2,
            use_cache=True
        )
    # è§£ç è¾“å‡º
    input_token_len = input_ids.shape[1]#2414
    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()
    if n_diff_input_output > 0:
        print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')
    outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
    #output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    outputs = outputs.strip()
    if outputs.endswith(stop_str):
        outputs = outputs[:-len(stop_str)]
    outputs = outputs.strip()    
    return outputs

# ç”¨äºæ˜¾ç¤ºå½“å‰å›¾ç‰‡çš„å‡½æ•°
def show_image(index):
    return images[index]

# æŒ‰é’®å‡½æ•°ï¼šåˆ‡æ¢åˆ°ä¸‹ä¸€å¼ å›¾ç‰‡
def next_image():
    global current_index
    if current_index < total_images - 1:
        current_index += 1
    return show_image(current_index)

# æŒ‰é’®å‡½æ•°ï¼šåˆ‡æ¢åˆ°ä¸Šä¸€å¼ å›¾ç‰‡
def prev_image():
    global current_index
    if current_index > 0:
        current_index -= 1
    return show_image(current_index)

# æŒ‰é’®å‡½æ•°ï¼šåˆ‡æ¢åˆ°ä¸‹ä¸€å¼ å›¾ç‰‡
def reset_image():
    global current_index
    current_index =0
    return show_image(current_index)


with gr.Blocks(css=custom_css) as demo:
    gr.Markdown("## å„¿ç«¥å™äº‹èƒ½åŠ›è¯„ä»·")
    with gr.Row():
        with gr.Column(scale=3):
            image_display = gr.Image(show_image(current_index),height=750, show_label=True,label='ã€Šé’è›™ï¼Œä½ åœ¨å“ªé‡Œï¼Ÿã€‹æ•…äº‹ä¹¦')#width=800, height=400
            with gr.Row():
                prev_btn = gr.Button(value="â¬…ï¸", elem_classes=["icon-btn"])
                next_btn = gr.Button(value="â¡ï¸", elem_classes=["icon-btn"])
                reset_btn = gr.Button(value="ğŸ”„", elem_classes=["icon-btn"])   
                # example_display  = gr.Examples(
                #         examples=truncated_text_examples,
                #         inputs=[input_text, generate_comment_checkbox, model_dropdown],  # å¯¹åº”è¾“å…¥ç»„ä»¶
                #         label="Example Inputs"  # ç¤ºä¾‹æ ‡ç­¾
                #     )     
        with gr.Column(scale=2):
            gr.Markdown("### è®²è®²ã€Šé’è›™ï¼Œä½ åœ¨å“ªé‡Œï¼Ÿã€‹çš„æ•…äº‹å§ï¼")
            with gr.Row():
               
                    # æ–‡ä»¶ä¸Šä¼ æˆ–éº¦å…‹é£å½•åˆ¶éŸ³é¢‘
                audio_input = gr.Audio(source="microphone", type="filepath", label="å½•åˆ¶æˆ–ä¸Šä¼ è¯­éŸ³",scale=3)
                record_button = gr.Button("è¯­éŸ³è¯†åˆ«",scale=1)

            with gr.Row():
                input_text = gr.Textbox(
                    placeholder="æˆ–ç”¨æ–‡å­—è®²è¿°è¿™ä¸ªæ•…äº‹",
                    lines=8,
                    show_label=False
                )
            with gr.Row():
            # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰èœå•
                model_dropdown = gr.Dropdown(
                    label="é€‰æ‹©æ¨¡å‹",
                    choices=list(model_paths.keys()),
                    value=first_model_item[0],
                    allow_custom_value=False,
                )
                # è¾“å‡ºåŠ è½½çŠ¶æ€
                load_status = gr.Textbox(value=f"åŠ è½½æ¨¡å‹æˆåŠŸï¼š{current_model_name}",
                                          interactive=False,
                                         show_label=False, 
                                        )
            # æ–°å¢â€œç”Ÿæˆè¯„è¯­â€å¤é€‰æ¡†
            with gr.Row():
                generate_comment_checkbox = gr.Checkbox(label="ç”Ÿæˆè¯„è¯­", value=False,visible=True)
                submit_btn = gr.Button("æäº¤")
    with gr.Row():
        gr.Markdown(CRITERIA,scale=2)
        # æ˜¾ç¤ºæ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬
        output_text = gr.Textbox(
            label="æ¨¡å‹è¯„ä»·",
            lines=12,
            interactive=False,  # è®¾ç½®ä¸ºä¸å¯ç¼–è¾‘
            show_copy_button=True,
            elem_classes=["textbox-custom"],
            scale=3
        )
    # è¯„ä»·éƒ¨åˆ†
    with gr.Row():
        with gr.Tabs():
            with gr.TabItem("å™äº‹å›¾"): 
                graph_display_plotly = gr.Plot(label='å™äº‹å›¾')
            with gr.TabItem("é‡‘æ ‡å›¾"):
                gold_graph_plotly =  gr.Plot(label='é‡‘æ ‡å™äº‹å›¾')
        #graph_display = gr.Image(show_label=True,label='å™äº‹å›¾')#width=800, height=500


    submit_btn.click(model_inference, inputs=[input_text,model_dropdown,generate_comment_checkbox], outputs=[output_text,graph_display_plotly,gold_graph_plotly])
    # å›¾ç‰‡åˆ‡æ¢æŒ‰é’®äº¤äº’é€»è¾‘
    # æŒ‰é’®äº¤äº’é€»è¾‘
    next_btn.click(next_image, outputs=image_display)
    prev_btn.click(prev_image, outputs=image_display)
    reset_btn.click(reset_image,outputs=image_display)
    
    # æ›´æ”¹æ¨¡å‹é€‰æ‹©æ—¶åŠ è½½æ–°æ¨¡å‹å¹¶æ˜¾ç¤ºçŠ¶æ€
    model_dropdown.change(fn=change_model, inputs=model_dropdown, outputs=load_status)
    # æŒ‰é’®ç‚¹å‡»è§¦å‘è¯­éŸ³è¯†åˆ«
    
    record_button.click(fn=recognize_audio, inputs=audio_input, outputs=input_text)

# å¯åŠ¨ Gradio åº”ç”¨
demo.launch(server_name="0.0.0.0", server_port=7860)


